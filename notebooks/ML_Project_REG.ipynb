{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9nDo_QpzEdrL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "dict_keys(['summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules', 'host_name', 'host_since', 'host_location', 'host_about', 'host_response_time', 'host_is_superhost', 'host_neighbourhood', 'host_has_profile_pic', 'host_identity_verified', 'neighbourhood', 'state', 'zipcode', 'market'])\n",
            "  Processing name...\n",
            "  Processing access...\n",
            "  Processing description...\n",
            "  Processing neighborhood_overview...\n",
            "  Processing interaction...\n",
            "  Processing house_rules...\n",
            "  Processing host_about...\n",
            "Huber Regression R²: 0.37394736490797753\n",
            "Huber Regression MAE: 2.362343820716811\n",
            "Huber Regression RMSE: 12.506947579458668\n",
            "Log-Transformed Regression R²: -460.0656048805365\n",
            "Linear = MSE: 11.7074, R²: 0.4140\n",
            "Polynomial = MSE: 11.2697, R²: 0.4359\n",
            "ElasticNet model R²: 0.4131\n",
            "ElasticNet model MAE: 2.4051\n",
            "Mean Squared Error (MSE): 11.9227\n",
            "Mean Absolute Error (MAE): 2.3371\n",
            "R² Score: 0.4032\n",
            "Decision_Tree R2_Test: 0.3993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\GHOST\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\GHOST\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but HuberRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "C:\\Users\\GHOST\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but ElasticNet was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFECV\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import hdbscan\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "from datetime import datetime\n",
        "import time\n",
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "import random\n",
        "import swifter\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "from tqdm_joblib import tqdm_joblib\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "\n",
        "##################################################################################################\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "df = pd.read_csv('test_data_reg.csv')    \n",
        "target_col = 'review_scores_rating'\n",
        "price_columns = ['nightly_price', 'price_per_stay', 'security_deposit', 'cleaning_fee', 'extra_people']\n",
        "for col in price_columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# 3. Convert percentage string to float\n",
        "if 'host_response_rate' in df.columns and df['host_response_rate'].dtype == 'object':\n",
        "    df['host_response_rate'] = df['host_response_rate'].str.replace('%', '').astype(float) / 100\n",
        "\n",
        "\n",
        "drop_cols = ['host_acceptance_rate', 'square_feet']\n",
        "df = df.drop(drop_cols, axis=1)\n",
        "df.drop(['host_listings_count'], axis=1, inplace=True)\n",
        "df.drop(\"thumbnail_url\", axis = 1, inplace = True)\n",
        "\n",
        "duplicate_count = df.duplicated().sum()\n",
        "if duplicate_count > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Duplicates dropped. New shape: {df.shape}\")\n",
        "\n",
        "#save scaler and imputer -1\n",
        "with open(r'D:\\fcis\\machine\\project\\pkls2\\modesToReplaceNulls.pkl', 'rb') as file:\n",
        "  modesForPickle =pickle.load(file)\n",
        "\n",
        "\n",
        "\n",
        "# Check the type and keys\n",
        "print(type(modesForPickle))\n",
        "if isinstance(modesForPickle, dict):\n",
        "    print(modesForPickle.keys())\n",
        "else:\n",
        "    print(\"The loaded object is not a dictionary.\")\n",
        "\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "for col in cat_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col] = df[col].fillna(modesForPickle[col])\n",
        "\n",
        "\n",
        "df.drop(\"zipcode\", axis = 1, inplace = True)\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "x = pd.DataFrame(num_cols, columns=['Numeric Columns'])\n",
        "\n",
        "# Create a scaler for all numeric columns\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\reg_scalerModel.pkl', 'rb') as file:\n",
        "  scaler= pickle.load(file)\n",
        "\n",
        "scaled_data = scaler.transform(df[num_cols])\n",
        "\n",
        "\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\re_imputerModel.pkl', 'rb') as file:\n",
        "  imputer=pickle.load(file)\n",
        "\n",
        "imputed_data = imputer.transform(scaled_data)\n",
        "\n",
        "df[num_cols] = pd.DataFrame(scaler.inverse_transform(imputed_data), columns=num_cols, index=df.index)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "skewed_cols = ['nightly_price', 'price_per_stay', 'number_of_reviews', 'host_total_listings_count', 'number_of_stays']\n",
        "\n",
        "for col in skewed_cols:\n",
        "    df[f'{col}'] = np.log1p(df[col])\n",
        "\n",
        "\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pkls2\\lowerUpperOutliersForPickle.pkl', 'rb') as file:\n",
        "    lowerUpperOutliers= pickle.load(file)\n",
        "\n",
        "\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=\"number\").columns\n",
        "\n",
        "for col in numeric_cols:\n",
        "    ##get the outlier bounds from pickle then filter\n",
        "    df[col] = np.where(df[col] < lowerUpperOutliers[col][0], lowerUpperOutliers[col][0], df[col])\n",
        "    df[col] = np.where(df[col] > lowerUpperOutliers[col][1], lowerUpperOutliers[col][1], df[col])\n",
        "\n",
        "host_total_listings_count = df.groupby('host_id')['host_id'].count().to_dict()\n",
        "df['host_experience'] = np.log1p(df['host_id'].map(lambda x: host_total_listings_count.get(x, 0)))  # Default to 0 if missing\n",
        "\n",
        "host_avg_price = df.groupby('host_id')['price_per_stay'].mean().to_dict()\n",
        "df['host_avg_price'] = df['host_id'].map(host_avg_price)\n",
        "\n",
        "host_total_reviews = df.groupby('host_id')['number_of_reviews'].sum().to_dict()\n",
        "df['host_total_reviews'] = df['host_id'].map(host_total_reviews)\n",
        "\n",
        "\n",
        "host_avg_reviews = df.groupby('host_id')['number_of_reviews'].mean()\n",
        "df['host_avg_reviews'] = df['host_id'].map(host_avg_reviews)\n",
        "\n",
        "host_avg_price = df.groupby('host_id')['nightly_price'].mean()\n",
        "df['host_avg_price'] = df['host_id'].map(host_avg_price)\n",
        "df['host_total_listings_count_log'] = np.log1p(df['host_total_listings_count'])\n",
        "\n",
        "df['host_experience'] = df['host_total_listings_count_log']\n",
        "df['price_per_stay'] = np.log1p(df['price_per_stay'])\n",
        "\n",
        "df['super_experience'] = df['host_experience'] * (df['host_is_superhost'] == 't').astype(int)\n",
        "df['reviews_per_stay'] = df['number_of_reviews'] / (df['number_of_stays'] + 1)\n",
        "df['price_per_person'] = df['price_per_stay'] / df['accommodates']\n",
        "df['price_diff'] = df['price_per_stay'] - df['nightly_price']\n",
        "\n",
        "host_avg_response = df.groupby('host_id')['host_response_rate'].mean()\n",
        "\n",
        "df['host_avg_response_rate'] = df['host_id'].map(host_avg_response)\n",
        "\n",
        "df['price_per_guest'] = df['price_per_stay'] / df['accommodates']\n",
        "\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pkls2\\LabelEncoder_for_host_is_superhost.pkl', 'rb') as file:\n",
        "    le= pickle.load(file)\n",
        "\n",
        "df['host_is_superhost'] = le.transform(df['host_is_superhost'])\n",
        "\n",
        "df['superhost_reviews_interaction'] = df['host_is_superhost'] * df['number_of_reviews']\n",
        "df['superhost_price_interaction'] = df['host_is_superhost'] * df['price_per_stay']\n",
        "\n",
        "df['superhost_experience_interaction'] = df['host_is_superhost'] * df['host_experience']\n",
        "df['superhost_reviews_interaction2'] = df['host_is_superhost'] * df['host_total_reviews']\n",
        "df['reviews_per_listing'] = df['host_total_reviews'] / (df['host_total_listings_count'] + 1)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "cluster_features = df[['host_total_listings_count', 'host_experience', 'host_total_reviews']].fillna(0)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\clusterScaled.pkl', 'rb') as file:\n",
        "    scaler=pickle.load(file)\n",
        "\n",
        "cluster_scaled = scaler.fit_transform(cluster_features)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\kmeanForHostCluster.pkl', 'rb') as file:\n",
        "    kmeans=pickle.load(file)\n",
        "\n",
        "df['host_cluster'] = kmeans.fit_predict(cluster_scaled)\n",
        "\n",
        "\n",
        "url_cols = ['listing_url', 'host_url']\n",
        "id_cols = ['id', 'host_id']\n",
        "cols_to_drop = id_cols + url_cols\n",
        "df.drop(cols_to_drop, axis = 1 , inplace = True)\n",
        "\n",
        "date_cols = ['host_since', 'first_review', 'last_review']\n",
        "\n",
        "for col in date_cols:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# Create features from dates\n",
        "reference_date = pd.Timestamp('2023-01-01')  # Use a fixed reference date for consistency\n",
        "for col in date_cols:\n",
        "    if col in df.columns:\n",
        "        df[f'{col}_days'] = (reference_date - df[col]).dt.days\n",
        "\n",
        "        # Extract month and year as cyclical features\n",
        "        if not df[col].isna().all():\n",
        "            df[f'{col}_month_sin'] = np.sin(2 * np.pi * df[col].dt.month / 12)\n",
        "            df[f'{col}_month_cos'] = np.cos(2 * np.pi * df[col].dt.month / 12)\n",
        "            df[f'{col}_year'] = df[col].dt.year\n",
        "\n",
        "        # Drop original date columns\n",
        "        df = df.drop(col, axis=1)\n",
        "\n",
        "\n",
        "text_cols = ['name','access' , 'description', 'neighborhood_overview',  'interaction', 'house_rules', 'host_about']\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def process_text(text):\n",
        "    if pd.isna(text):\n",
        "        return {\n",
        "            'word_count': 0,\n",
        "            'sentiment_compound': 0,\n",
        "            'sentiment_pos': 0,\n",
        "            'sentiment_neg': 0,\n",
        "            'sentiment_neu': 0\n",
        "        }\n",
        "\n",
        "    # Clean text\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Count words\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    # Get sentiment\n",
        "    sentiment = sia.polarity_scores(text)\n",
        "\n",
        "    return {\n",
        "        'word_count': word_count,\n",
        "        'sentiment_compound': sentiment['compound'],\n",
        "        'sentiment_pos': sentiment['pos'],\n",
        "        'sentiment_neg': sentiment['neg'],\n",
        "        'sentiment_neu': sentiment['neu']\n",
        "    }\n",
        "\n",
        "def process_column(df, col):\n",
        "    print(f\"  Processing {col}...\")\n",
        "    text_features = df[col].apply(process_text)  # Apply process_text to each element\n",
        "\n",
        "    # Convert list of dicts to dataframe\n",
        "    text_features_df = pd.DataFrame(text_features.tolist())\n",
        "\n",
        "    # Add column prefix\n",
        "    text_features_df.columns = [f\"{col}_{feat}\" for feat in text_features_df.columns]\n",
        "\n",
        "    return text_features_df\n",
        "\n",
        "def apply_tfidf(df, col, max_features=20):\n",
        "    MODEL_DIR = \"mo/mo\"\n",
        "    print(f\"Applying TF-IDF on: {col}\")\n",
        "\n",
        "    # Load the saved TF-IDF vectorizer\n",
        "    tfidf = joblib.load(os.path.join(MODEL_DIR, f\"{col}_tfidf_vectorizer.joblib\"))\n",
        "\n",
        "    # Fill NaNs with empty strings and transform using the loaded vectorizer\n",
        "    tfidf_matrix = tfidf.transform(df[col].fillna(\"\"))\n",
        "\n",
        "    # Load the column names\n",
        "    tfidf_columns = joblib.load(os.path.join(MODEL_DIR, f\"{col}_tfidf_columns.joblib\"))\n",
        "\n",
        "    # Create a DataFrame from TF-IDF matrix with the loaded column names\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_columns)\n",
        "\n",
        "    return tfidf_df\n",
        "\n",
        "# Process each text column for sentiment analysis and word count\n",
        "for col in text_cols:\n",
        "    if col in df.columns:\n",
        "        text_features_df = process_column(df, col)\n",
        "\n",
        "        df = pd.concat([df, text_features_df], axis=1)\n",
        "\n",
        "        #tfidf_df = apply_tfidf(df, col)\n",
        "\n",
        "        #df = pd.concat([df, tfidf_df], axis=1)\n",
        "\n",
        "        df = df.drop(col, axis=1)\n",
        "\n",
        "\n",
        "df['host_sentiment_label'] = df['host_about_sentiment_compound'].apply(\n",
        "    lambda x: 'positive' if x > 0.5 else 'neutral' if x > 0 else 'negative'\n",
        ")\n",
        "\n",
        "df['overall_sentiment_score'] = (\n",
        "    df[['host_about_sentiment_compound']].mean(axis=1)\n",
        ")\n",
        "\n",
        "cols_to_drop = [\n",
        "    col for col in df.columns\n",
        "    if col.endswith('_sentiment_neg') or\n",
        "       col.endswith('_sentiment_neu') or\n",
        "       col.startswith('name_sentiment')\n",
        "]\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Interaction between reviews and host features\n",
        "if 'number_of_reviews' in df.columns and 'host_is_superhost' in df.columns:\n",
        "    # Convert binary text values to numeric if needed\n",
        "    if df['host_is_superhost'].dtype == 'object':\n",
        "        df['host_is_superhost'] = df['host_is_superhost'].map({'t': 1, 'f': 0})\n",
        "\n",
        "    df['superhost_review_interaction'] = df['host_is_superhost'] * df['number_of_reviews']\n",
        "\n",
        "df['price_per_person'] = np.log1p(df['price_per_person'])\n",
        "df = df.drop(['amenities'], axis=1)\n",
        "df.drop(['property_type'], axis=1, inplace=True)\n",
        "df.drop(['host_has_profile_pic', 'require_guest_profile_picture','require_guest_phone_verification','requires_license','is_business_travel_ready' ], axis=1, inplace=True)\n",
        "\n",
        "binary_cols = ['host_is_superhost',  'host_identity_verified',\n",
        "               'is_location_exact', 'instant_bookable']\n",
        "for col in binary_cols:\n",
        "    if col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].map({'t': 1, 'f': 0})\n",
        "\n",
        "\n",
        "df = df.drop('city', axis=1)\n",
        "\n",
        "# # Price-related ratios\n",
        "if all(col in df.columns for col in ['price_per_stay', 'nightly_price', 'cleaning_fee']):\n",
        "    df['price_to_nightly_ratio'] = df['price_per_stay'] / (df['nightly_price'] + 0.001)  # Avoid division by zero\n",
        "\n",
        "# Reviews-related ratios\n",
        "if all(col in df.columns for col in ['number_of_reviews', 'number_of_stays']):\n",
        "    df['review_rate'] = df['number_of_reviews'] / (df['number_of_stays'] + 0.001)\n",
        "\n",
        "# 9. Remove high cardinality categorical columns\n",
        "high_cardinality_cols = ['host_location', 'host_neighbourhood', 'country', 'city', 'street',\n",
        "                         'neighbourhood', 'neighbourhood_cleansed', 'zipcode', 'market', 'smart_location']\n",
        "\n",
        "for col in high_cardinality_cols:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(col, axis=1)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\host_name_target_means.pkl', 'rb') as file:\n",
        "            target_means= pickle.load(file)\n",
        "\n",
        "df['host_name_target_encoded'] = df['host_name'].map(target_means)\n",
        "with open(r'D:\\fcis\\machine\\project\\pkls2\\host_name_value_counts.pkl', 'rb') as file:\n",
        "            value_counts= pickle.load(file)\n",
        "\n",
        "df['host_name_freq_encoded'] = df['host_name'].map(value_counts)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\selected_features.pkl', 'rb') as file:\n",
        "    selected_features= pickle.load(file)\n",
        "\n",
        "df = df[selected_features.tolist() + [target_col]]\n",
        "\n",
        "bool_cols = df.select_dtypes(include=['bool']).columns\n",
        "\n",
        "# Convert boolean columns to integers (0 for False, 1 for True)\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\scaleFeatures.pkl', 'rb') as file:\n",
        "    scaler= pickle.load(file)\n",
        "\n",
        "X_scaled = scaler.transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features)\n",
        "\n",
        "#models\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\linearRegression.pkl', 'rb') as file:\n",
        "    lin_reg= pickle.load(file)\n",
        "\n",
        "\n",
        "y_pred_test = lin_reg.predict(X_scaled)\n",
        "\n",
        "r2_LR = r2_score(y, y_pred_test)\n",
        "mse = mean_squared_error(y, y_pred_test)\n",
        "mae = mean_absolute_error(y, y_pred_test)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\huberModel.pkl', 'rb') as file:\n",
        "    huber= pickle.load(file)\n",
        "\n",
        "y_pred_huber = huber.predict(X_scaled)\n",
        "\n",
        "r2_hyper = r2_score(y, y_pred_huber)\n",
        "print(\"Huber Regression R²:\",r2_hyper )\n",
        "print(\"Huber Regression MAE:\", mean_absolute_error(y, y_pred_huber))\n",
        "print(\"Huber Regression RMSE:\", mean_squared_error(y, y_pred_huber))#, squared=False\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\linearLogTransformation.pkl', 'rb') as file:\n",
        "    lin_reg=pickle.load(file)\n",
        "\n",
        "y_pred_log = lin_reg.predict(X)\n",
        "y_pred_exp = np.exp(y_pred_log)\n",
        "print(\"Log-Transformed Regression R²:\", r2_score(y, y_pred_exp))\n",
        "\n",
        "\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\LinearOriginal.pkl', 'rb') as file:\n",
        "    model_original=pickle.load(file)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\Log Transform.pkl', 'rb') as file:\n",
        "    model_log= pickle.load(file)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\Sqrt Transform.pkl', 'rb') as file:\n",
        "    model_sqrt=pickle.load(file)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\Polynomial.pkl', 'rb') as file:\n",
        "    model_poly= pickle.load(file)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\HuberModel2.pkl', 'rb') as file:\n",
        "    model_robust =pickle.load(file)\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\RANSAC.pkl', 'rb') as file:\n",
        "    model_ransac= pickle.load(file)\n",
        "\n",
        "\n",
        "models = {\n",
        "    'Linear': model_original,\n",
        "    # 'Log Transform': model_log,\n",
        "    # 'Sqrt Transform': model_sqrt,\n",
        "    'Polynomial': model_poly,\n",
        "    # 'Huber': model_robust,\n",
        "    # 'RANSAC': model_ransac\n",
        "}\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\X_polyTransform.pkl', 'rb') as file:\n",
        "    poly= pickle.load(file)\n",
        "\n",
        "X_poly = poly.transform(X)\n",
        "\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name in ['Log Transform', 'Sqrt Transform']:\n",
        "        if name == 'Log Transform':\n",
        "            preds = np.expm1(model.predict(X))\n",
        "        else:\n",
        "            preds = model.predict(X) ** 2\n",
        "    else:\n",
        "        if name == 'Polynomial':\n",
        "            preds = model.predict(X_poly)\n",
        "        elif name =='Log-Transformed Regression':\n",
        "          continue\n",
        "        else:\n",
        "            preds = model.predict(X)\n",
        "\n",
        "    mse = mean_squared_error(y, preds)\n",
        "    r2 = r2_score(y, preds)\n",
        "\n",
        "    print(f\"{name} = MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\elastic_net_model.pkl', 'rb') as file:\n",
        "    elastic_net_model=pickle.load(file)\n",
        "\n",
        "predict_en = elastic_net_model.predict(X_scaled)\n",
        "\n",
        "r2_en = r2_score(y, predict_en)\n",
        "mse_en = mean_squared_error(y, predict_en)\n",
        "mae_en = mean_absolute_error(y, predict_en)\n",
        "\n",
        "print(\"ElasticNet model R²: {:.4f}\".format(r2_en))\n",
        "print(\"ElasticNet model MAE: {:.4f}\".format(mae_en))\n",
        "\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\best_DT_model.pkl', 'rb') as file:\n",
        "    best_DT_model= pickle.load(file)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_DT_model.predict(X)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "with open(r'D:\\fcis\\machine\\project\\pickleFiles\\DecisionTreeRegressor.pkl', 'rb') as file:\n",
        "    reg= pickle.load(file)\n",
        "y_pred_test = reg.predict(X)\n",
        "# R² Scores\n",
        "\n",
        "print('Decision_Tree R2_Test: {0:0.4f}'.format(r2_score(y, y_pred_test)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
